{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.read_csv(\"https://docs.google.com/spreadsheets/d/e/2PACX-1vQtBXo5cBnDsM2fmfHPm6u72KGUS5FjPHNGMxOfYjA9-CAhmnRpwkIw_rOR3sANJIToiUU__6fbBvig/pub?gid=572763137&single=true&output=csv\")"
      ],
      "metadata": {
        "id": "UUVYvb6V4XGC"
      },
      "execution_count": 372,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#finding number of null values per column\n",
        "data.isnull().sum()\n",
        "#filling null values with median of each column\n",
        "data.fillna(data.median(numeric_only=True), inplace=True)\n",
        "\n",
        "#Seprating Inputs and Outputs\n",
        "X=data.drop(columns=['Time','Pass/Fail'])\n",
        "y=data['Pass/Fail']\n",
        "\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SYafOb_a-KA3",
        "outputId": "6d1be2bc-ca01-4dfa-bb1f-bb9bdc2621f0"
      },
      "execution_count": 373,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1567, 590)"
            ]
          },
          "metadata": {},
          "execution_count": 373
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dropping Constant Columns before Stadardization\n",
        "X = X.loc[:, X.std()!=0]\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL2DSfnNHbin",
        "outputId": "37376211-d6b4-4d93-f661-7c563ff75701"
      },
      "execution_count": 374,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1567, 474)"
            ]
          },
          "metadata": {},
          "execution_count": 374
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n"
      ],
      "metadata": {
        "id": "SLaxSiF1Isr8"
      },
      "execution_count": 375,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr = LogisticRegression()\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "y_pred = lr.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "OxF27_WOMN2V"
      },
      "execution_count": 376,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using error rate as evaluation metrics because it was ordered to do so but observe data is imbalanced so we should opt for precision, f1 score, roc auc curve\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "accuracy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55TL4MM0Mic6",
        "outputId": "4d906fa3-e9a4-4dbd-f5cc-838138b6f811"
      },
      "execution_count": 377,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8821656050955414"
            ]
          },
          "metadata": {},
          "execution_count": 377
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_scaled.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1WCbMCsRXQm",
        "outputId": "e21d0c0a-edd4-418b-e4e4-e3f9c9e7c07f"
      },
      "execution_count": 378,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1253, 474)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now We will apply filter based feature selection techniques to reduce number of features and still get approximately same error **"
      ],
      "metadata": {
        "id": "HlnCy1ZoNhsT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Deleting Duplicate Columns **\n",
        "\n"
      ],
      "metadata": {
        "id": "X6uRoQUWN04o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_duplicate_columns(df):\n",
        "\n",
        "    duplicate_columns = {}\n",
        "    seen_columns = {}\n",
        "\n",
        "    for column in df.columns:\n",
        "        current_column = df[column]\n",
        "\n",
        "        # Convert column data to bytes\n",
        "        try:\n",
        "            current_column_hash = current_column.values.tobytes()\n",
        "        except AttributeError:\n",
        "            current_column_hash = current_column.to_string().encode()\n",
        "\n",
        "        if current_column_hash in seen_columns:\n",
        "            if seen_columns[current_column_hash] in duplicate_columns:\n",
        "                duplicate_columns[seen_columns[current_column_hash]].append(column)\n",
        "            else:\n",
        "                duplicate_columns[seen_columns[current_column_hash]] = [column]\n",
        "        else:\n",
        "            seen_columns[current_column_hash] = column\n",
        "\n",
        "    return duplicate_columns\n",
        "duplicate_columns = get_duplicate_columns(X_train_scaled)"
      ],
      "metadata": {
        "id": "55l4FWDmNw3E"
      },
      "execution_count": 379,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(duplicate_columns) # key and its corresponding values are duplicate columns\n",
        "print(X_train_scaled.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UDqnP7zSc9x",
        "outputId": "41e30904-2925-4976-dcfc-25675d363208"
      },
      "execution_count": 380,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'74': ['209', '342'], '206': ['347', '478']}\n",
            "(1253, 474)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#dropping duplicate columns from train and test data\n",
        "\n",
        "for drop_list in duplicate_columns.values():\n",
        "  X_train_scaled =  X_train_scaled.drop(columns=drop_list)\n",
        "  X_test_scaled =  X_test_scaled.drop(columns=drop_list)\n"
      ],
      "metadata": {
        "id": "_cx_4DNkOYnO"
      },
      "execution_count": 381,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_scaled.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JYQQnvVRblC",
        "outputId": "6e752271-7374-46d7-a110-699bad180d68"
      },
      "execution_count": 382,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1253, 470)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Till Now 4 features has been dropped."
      ],
      "metadata": {
        "id": "v5osNJO5SuCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Variance Threshold Method**"
      ],
      "metadata": {
        "id": "Ift1t6vJSzST"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "\n",
        "selector = VarianceThreshold(threshold=0.01)\n",
        "sel = selector.fit(X_train_scaled)\n",
        "column=X_train_scaled.columns[sel.get_support()]\n",
        "\n",
        "X_train_scaled = sel.transform(X_train_scaled)\n",
        "X_test_scaled = sel.transform(X_test_scaled)\n",
        "\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns = column)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=column)\n",
        "X_train_scaled.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYbro9sUS6fb",
        "outputId": "8a39b2cc-5151-4a8a-dd56-0bc89fcb4af8"
      },
      "execution_count": 383,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1253, 470)"
            ]
          },
          "metadata": {},
          "execution_count": 383
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variance Threshold Method didn't make any difference since all column passed this test."
      ],
      "metadata": {
        "id": "r6pOHO2QWGog"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Correlation Method**"
      ],
      "metadata": {
        "id": "Kozh4blEWOEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corr_matrix = X_train_scaled.corr()\n",
        "columns= corr_matrix.columns\n",
        "to_drop=[]\n",
        "for i in range(len(columns)):\n",
        "  for j in range(i+1, len(columns)):\n",
        "    if abs(corr_matrix.loc[columns[i], columns[j]])>0.9:\n",
        "      to_drop.append(columns[j])\n",
        "\n",
        "to_drop = set(to_drop) #since repeating values\n",
        "\n",
        "X_train_scaled.drop(columns=to_drop, axis=1, inplace=True)\n",
        "X_test_scaled.drop(columns=to_drop, axis=1, inplace=True)\n",
        "\n",
        "X_train_scaled.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTKPMePBWS8P",
        "outputId": "3a2c7b1a-0efc-4f29-abbf-cc3db3625cfc"
      },
      "execution_count": 384,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1253, 264)"
            ]
          },
          "metadata": {},
          "execution_count": 384
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANOVA Method**"
      ],
      "metadata": {
        "id": "w5bJKOobZafY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import f_classif\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "\n",
        "sele = SelectKBest(f_classif, k=100).fit(X_train_scaled, y_train)\n",
        "\n",
        "# display selected feature names\n",
        "X_train_scaled.columns[sele.get_support()]\n",
        "\n",
        "columns = X_train_scaled.columns[sele.get_support()]\n",
        "X_train_scaled = sele.transform(X_train_scaled)\n",
        "X_test_scaledd = sele.transform(X_test_scaled)\n",
        "\n",
        "X_train_scaled = pd.DataFrame(X_train_scaled, columns=columns)\n",
        "X_test_scaled = pd.DataFrame(X_test_scaled, columns=columns)\n",
        "print(X_train_scaled.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYRsKHODZgbs",
        "outputId": "b430a76d-ecf4-4352-b600-fc9e1546bdd5"
      },
      "execution_count": 385,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1253, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Now Moment Of Truth : To check if error remains approximately same or not ***"
      ],
      "metadata": {
        "id": "IsuKKKN3bI3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying logistic Regression\n",
        "lr1 = LogisticRegression()\n",
        "lr1.fit(X_train_scaled, y_train)\n",
        "y_pred = lr1.predict(X_test_scaled)"
      ],
      "metadata": {
        "id": "WXI5ZuyTbS_A"
      },
      "execution_count": 386,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using error rate as evaluation metrics because it was ordered to do so but observe data is imbalanced so we should opt for precision, f1 score, roc auc curve\n",
        "accuracy1 = accuracy_score(y_test, y_pred)\n",
        "error = 1-accuracy1\n",
        "accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgGDH50KbUwA",
        "outputId": "8a014356-d0e2-4660-cbbb-423a31e2c04e"
      },
      "execution_count": 387,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8821656050955414"
            ]
          },
          "metadata": {},
          "execution_count": 387
        }
      ]
    }
  ]
}